import logging
import re
import copy
import pandas as pd
import random
import pprint
import networkx as nx
import itertools
import numpy as np
from pgmpy.base import DAG
from pgmpy.models import DiscreteBayesianNetwork
from pgmpy.estimators import BIC
from pgmpy.estimators import BDeu
from typing import Tuple
import itertools
from collections import deque
from joblib import Parallel, delayed  # Parallelization for candidate evaluation

from llm import LLMClient
from .base import BaseRefiner
from errors.agent_error import StateUpdateError
from utils.eval_utils import evaluate_generation
from utils.graph_utils import generation_dict_to_discrete_bn, initialize_empty_graph, adjacency_df_to_bn, evaluate_operation
from utils.score_utils import LocalScoreCache

THRESHOLD = 1e-6
MAXIMUM_ACTION_COUNT = 10
PLATEAU_COUNT = 3


class ReActBNAgent(BaseRefiner):
    """
    ReActBN: LLM-Enhanced Bayesian Network Structure Refiner for BNSynth.

    This agent implements the ReAct (Reason + Act) framework for intelligent BN structure
    refinement. It combines traditional hill climbing search with LLM-enhanced reasoning
    to make more informed decisions about graph modifications, enabling data-dependent
    structure optimization with intelligent guidance.
    
    The agent alternates between:
    - Reasoning: Using LLM to understand the current state and evaluate actions
    - Acting: Performing graph operations based on LLM guidance
    
    This LLM-enforced approach leverages domain knowledge and adapts to different problem
    contexts more effectively than pure statistical algorithms.
    
    Key features:
    - ReAct framework for iterative reasoning and action
    - LLM-guided decision making with domain knowledge  
    - Support for various scoring functions (BIC, BDeu)
    - Top-k candidate selection for better exploration
    - Search history tracking for informed decisions
    - Flexible action spaces and constraints
    """

    def __init__(
        self,
        model: str,
        logger: logging.Logger,
    ):
        super().__init__(logger)
        self.scorer = None
        self.llm_client = LLMClient.from_model(model, logger)
        self.nodes = None
        self.ref_graph = None
        self.history = []

    @property
    def name(self):
        return "ReActBNAgent"

    def reason_and_act(
        self,
        state: dict,
        action_space: list,
    ) -> Tuple[str, int, float]:
        """
        Use LLM to generate reasoning and decide on an action.
        Returns (reasoning, action_idx, confidence).
        """
        prompt = self._format_llm_prompt(state, action_space)
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ]
        llm_response = self.llm_client.chat(messages)
        action_idx, reasoning, confidence = self._parse_llm_response(
            llm_response,
        )
        return reasoning, action_idx, confidence

    def _format_llm_prompt(self, state: dict, action_space: list) -> str:
        """
        Formats the prompt for the LLM, including the current graph, score, action space, variable descriptions, and history.
        """
        graph_edges = list(state["graph"].edges())
        score = state["score"]
        desc_variables = state.get("desc_variables", "")
        history = state.get("history", [])
        prompt = (
            "You are optimizing a Bayesian network structure. You will be given:\n"
            f"[Variables(Node) information]:\n{desc_variables}\n"
            f"[Current {self.scoring_method} score]: {score}\n"
            f"[Current edges]: {graph_edges}\n"
            "You will need to select the the next best action from the following candidate actions:\n"
            "[Candidate actions]:\n"
        )
        for h in history[-5:]:
            prompt += f"- {h}\n"
        prompt += f"Top-{len(action_space)} candidate actions (with score delta):\n"
        for idx, action in enumerate(action_space):
            prompt += f"Action {idx}: {action['type']} edge {action['from']} -> {action['to']} (score delta: {action['score_delta']:.4f})\n"
        prompt += (
            "\n"
            "The score delta represents the improvement in the score after applying the action. The score represents the observation data, not the true distribution.\n"
            "Think step by step. Reason carefully and wisely based on your knowledge given the information of the variables.\n"
            "Take the action you are most confident about with reasoning.\n"
            "If you believe none of the candidate actions are beneficial, return the Action index: -1.\n"
            "[Response]\n[Reasoning]: <your reasoning>.\n[Action index]: <number>.\n[Confidence]: <0-1>."
        )
        return prompt

    def _parse_llm_response(self, response: str) -> Tuple[int, str, float]:
        """
        Parses the LLM response to extract the action index, reasoning, and confidence.
        """
        match = re.search(r"\[Action index\]:\s*(-?\d+)", response)
        action_idx = int(match.group(1)) if match else 0
        confidence_match = re.search(r"\[Confidence\]:\s*([0-9.]+)", response)
        confidence = float(confidence_match.group(1)) if confidence_match else 0.0
        reasoning_match = re.search(
            r"\[Reasoning\]:(.*?)(?:\[Action index\]:|$)",
            response,
            re.DOTALL,
        )
        reasoning = reasoning_match.group(1).strip() if reasoning_match else response.strip()
        return action_idx, reasoning, confidence

    def _load_scorer(self, scoring_method: str, observation: pd.DataFrame) -> None:
        self.scoring_method = scoring_method
        if scoring_method.lower() == 'bic':
            self.scorer = BIC(observation)
        elif scoring_method.lower() == 'bdeu':
            self.scorer = BDeu(observation)
        else:
            raise ValueError(
                f"Unknown scoring_method: {scoring_method}. Use 'bic' or 'bdeu'.")

    def _initialize_state(self, current_graph, desc_variables):
        """
        Explicitly initialize the state dictionary before the main loop.
        """
        initial_nhd, initial_shd = self._compute_and_log_metrics(
            current_graph, iteration=0, current_score=0.0, confidence=None
        )
        state = {
            'graph': current_graph,
            'score': 0.0,
            'dag_variables': self.nodes,
            'desc_variables': desc_variables,
            'move_count': 0,
            'init_shd': initial_shd,
            'init_nhd': initial_nhd,
            'shd': initial_shd,
            'nhd': initial_nhd,
            'action_history': [],
            'metrics_history': [],
        }
        if not state['action_history']:
            state['action_history'].append({
                "action": None,
                "reward": 0.0,
                "score": state['score'],
                "shd": state['shd'],
                "nhd": state['nhd'],
                "iteration": 0,
                "terminated": False
            })
        return state

    def run(
        self,
        observation: pd.DataFrame,
        desc_variables: str,
        dag_variables: list,
        dag: pd.DataFrame,
        init_generation: dict,
        tabu_length: int = 100,
        epsilon: float = 1e-4,
        max_iter: int = 20,
        max_indegree: int = None,
        forbidden_edges: list = None,
        required_edges: list = None,
        scoring_method: str = 'bic',
        top_k: int = 10,
    ) -> dict:
        """
        Optimized custom hill climbing structure learning with tabu, local scoring, and score caching.
        Supports:
        - max_indegree: maximum number of parents per node (int or None)
        - forbidden_edges: list of (from, to) tuples that cannot be added/flipped
        - required_edges: list of (from, to) tuples that cannot be removed/flipped
        - start_dag: initial graph (as DiscreteBayesianNetwork or compatible dict)
        - scoring_method: 'bic' or 'bdeu'
        - top_k: number of top candidate actions to present to the LLM at each step
        """
        self.history.clear()  # Clear action history
        self.nodes = dag_variables
        self.ref_graph = adjacency_df_to_bn(dag, dag_variables)
        
        # Algorithm parameters
        self._load_scorer(scoring_method, observation)  # Load scorer
        forbidden_edges = set(forbidden_edges) if forbidden_edges else set()
        required_edges = set(required_edges) if required_edges else set()
        tabu_list = deque(maxlen=tabu_length)

        # Initialize graph
        if init_generation is not None:
            current_graph = generation_dict_to_discrete_bn(
                init_generation,
                dag_variables,
            )
            for node in self.nodes:
                current_graph.add_node(node)
        else:
            current_graph = initialize_empty_graph(dag_variables)

        # Initialize score cache
        cache = LocalScoreCache()

        # Initialize score
        current_score = 0.0
        for node in self.nodes:
            parents = set(current_graph.predecessors(node))
            current_score += cache.get_local_score(node, parents, self.scorer)

        # Initialize state
        state = self._initialize_state(current_graph, desc_variables)
        
        # Initialize iteration conditions
        iteration = 0
        done = False
        best_op = None
        best_score_delta = None
        self.logger.info(
            f"Starting optimized hill climbing with tabu (max_iter={max_iter}, epsilon={epsilon}, parallel=joblib)")
        metrics_history = []
        
        # --- Main loop: Hill climbing ---
        while not done and iteration < max_iter:
            iteration += 1
            # --- Main loop: Generate all legal operations ---
            legal_ops = self._generate_legal_operations(
                state['graph'], self.nodes, tabu_list, max_indegree, forbidden_edges, required_edges
            )
            # --- Main loop: Evaluate all legal operations in parallel using joblib ---
            def evaluate_op(op):
                return evaluate_operation(state['graph'], op, cache, self.scorer, state['score'])
            if legal_ops:
                results = Parallel(n_jobs=-1, prefer='threads')(
                    delayed(evaluate_op)(op) for op in legal_ops
                )
                op_results = list(zip(legal_ops, results))
                op_results.sort(key=lambda x: x[1], reverse=True)
                top_k_ops = op_results[:top_k]
                action_space = [
                    {
                        "type": op[0],
                        "from": op[1][0],
                        "to": op[1][1],
                        "score_delta": res
                    }
                    for op, res in top_k_ops
                ]
                state_for_llm = {
                    "graph": state['graph'],
                    "score": state['score'],
                    "desc_variables": state['desc_variables'],
                    "history": list(self.history),
                }
                reasoning, action_idx, confidence = self.reason_and_act(
                    state_for_llm,
                    action_space,
                )
                if action_idx == -1:
                    self.logger.info(
                        f"Iteration {iteration}: LLM called for termination (Action index: -1). Confidence: {confidence}"
                    )
                    self.logger.debug(f"Reasoning: {reasoning}")
                    current_nhd, current_shd = self._compute_and_log_metrics(
                        state['graph'], iteration, state['score'], None,
                    )
                    reward = None
                    self._log_step(
                        metrics_history=metrics_history,
                        action_history=self.history,
                        iteration=iteration,
                        current_score=state['score'],
                        current_nhd=current_nhd,
                        current_shd=current_shd,
                        action=None, action_idx=None, reward=None, confidence=None, reasoning=None, legal_ops_len=0,
                        terminated=True,
                    )
                    done = True
                    action = None
                    break
                elif action_idx >= len(top_k_ops):
                    self.logger.info(
                        f"Invalid action index: {action_idx}. Skipping this iteration."
                    )
                    self._log_step(
                        metrics_history=metrics_history,
                        action_history=self.history,
                        iteration=iteration,
                        current_score=state['score'],
                        current_nhd=current_nhd,
                        current_shd=current_shd,
                        action=None, action_idx=None, reward=None, confidence=None, reasoning="LLM responded with an invalid action index. Skip this iteration.", legal_ops_len=len(legal_ops),
                        terminated=False,
                    )
                    continue
                chosen_op, chosen_result = top_k_ops[action_idx]
                best_score_delta, best_op, best_new_score = chosen_result[
                    0], chosen_op, chosen_result[2]
                self.logger.info(
                    f"Iteration {iteration}: selected_op={best_op}, selected_action_idx={action_idx}, score_delta={best_score_delta}, confidence={confidence}"
                )
                self.logger.debug(f"Reasoning: {reasoning}")
            else:
                best_op = None
                best_score_delta = float('-inf')
                reward = best_score_delta
                best_new_score = None
                self.logger.info(f"Iteration {iteration}: no legal ops found")
                current_nhd, current_shd = self._compute_and_log_metrics(
                    state['graph'], iteration, state['score'], None,
                )
                self._log_step(
                    metrics_history=metrics_history,
                    action_history=self.history,
                    iteration=iteration,
                    current_score=state['score'],
                    current_nhd=current_nhd,
                    current_shd=current_shd,
                    action=None, action_idx=None, reward=None, confidence=None, reasoning=None, legal_ops_len=0, terminated=True,
                )
                break

            # --- Main loop: Stopping condition ---
            if best_op is None or best_score_delta < epsilon:
                self.logger.info(
                    f"Stopping: no operation improves score by more than epsilon ({epsilon}) or no legal ops.")
                done = True
                action = None
                break
            
            # --- Main loop: Apply best operation in-place ---
            self._apply_operation_in_place(state, best_op, tabu_list)
            
            # --- Main loop: Update state ---
            op_type, (parent, child) = best_op
            action = {'type': op_type, 'from': parent, 'to': child}
            for node in self.nodes:
                if node not in state['graph']:
                    state['graph'].add_node(node)
            current_nhd, current_shd = self._compute_and_log_metrics(
                state['graph'], iteration, best_new_score, confidence,
            )
            action_result = {
                'graph': state['graph'],
                'score': best_new_score,
                'shd': current_shd,
                'nhd': current_nhd,
            }
            reward = best_score_delta
            state = self.update_state(
                state,
                action,
                action_result,
                reward,
            )
            # --- Main loop: Log step ---
            self._log_step(
                metrics_history=metrics_history,
                action_history=self.history,
                iteration=iteration,
                current_score=state['score'],
                current_nhd=current_nhd,
                current_shd=current_shd,
                action=action,
                action_idx=action_idx,
                reward=reward,
                confidence=confidence,
                reasoning=reasoning,
                legal_ops_len=len(legal_ops),
                terminated=False,
            )
        self.logger.info(
            f"Hill climbing finished after {iteration} iterations. Final score: {state['score']}")
        self.logger.info(
            f"Local score cache: {cache.hits} hits, {cache.misses} misses.")
        result = self._finalize_result(
            state, action, state['graph'], state['score'], state['shd'], state['nhd'], iteration, reward, metrics_history,
        )
        result["ActionHistory"] = self.history
        return result

    def _select_action(self, action_space: list, state: dict) -> dict:
        # for each action, get the new graph and compute graph bic score
        delta_bic = {}
        for idx, action in enumerate(action_space):
            action_result = self.execute_action(action, state)
            delta_bic[idx] = action_result["score"] - state["score"]
        # select the action with the highest delta BIC score
        return action_space[max(delta_bic, key=delta_bic.get)]

    def _local_score_graph(self, graph: DiscreteBayesianNetwork) -> dict:
        """
        Computes the local score of each node in a graph using the stored scorer.
        """
        local_scores = {}
        for node in graph.nodes():
            parents = graph.get_parents(node)
            local_scores[node] = self.scorer.local_score(node, parents=parents)
        return local_scores

    def _score_graph(self, graph: DiscreteBayesianNetwork) -> float:
        """
        Computes the score of a graph using the stored scorer.
        """
        return self.scorer.score(graph)

    def _get_action_space(self, graph: DiscreteBayesianNetwork) -> list:
        """
        Returns a list of all valid actions (add, delete, reverse) for the current graph.
        Each action is a dict: {"type": ..., "from": ..., "to": ...}
        """
        actions = []
        nodes = list(graph.nodes())
        edges = set(graph.edges())

        # Add actions
        for i in nodes:
            for j in nodes:
                if i == j or (i, j) in edges:
                    continue
                # Check if adding (i, j) creates a cycle
                try:
                    graph.add_edge(i, j)
                except:
                    pass

        # Delete actions
        for (i, j) in edges:
            actions.append({"type": "delete", "from": i, "to": j})

        # Reverse actions
        for (i, j) in edges:
            if (j, i) in edges or i == j:
                continue
            # Check if reversing (i, j) creates a cycle
            graph.remove_edge(i, j)
            try:
                graph.add_edge(j, i)
            except:
                graph.add_edge(i, j)

        return actions

    def execute_action(self, action: dict, state: dict) -> dict:
        """
        Applies the selected action to the current graph and returns the new graph and its score.
        """
        graph = state["graph"]
        new_graph = copy.deepcopy(graph)
        action_type = action["type"]
        from_node = action["from"]
        to_node = action["to"]

        if action_type == "add":
            new_graph.add_edge(from_node, to_node)
        elif action_type == "delete":
            if new_graph.has_edge(from_node, to_node):
                new_graph.remove_edge(from_node, to_node)
        elif action_type == "reverse":
            if new_graph.has_edge(from_node, to_node):
                new_graph.remove_edge(from_node, to_node)
                new_graph.add_edge(to_node, from_node)
        else:
            raise ValueError(f"Unknown action type: {action_type}")

        # Compute the new score
        new_score = self._score_graph(new_graph)
        return {"graph": new_graph, "score": new_score}

    def get_reward(self, action_result: dict, state: dict) -> float:
        """
        Computes the reward as the change in score after taking the action.
        """
        new_score = action_result["score"]
        current_score = state["score"]
        return new_score - current_score

    def update_state(
        self,
        state: dict,
        action: dict,
        action_result: dict,
        reward: float,
    ) -> dict:
        new_state = state.copy()
        new_state["graph"] = action_result["graph"]
        new_state["score"] = action_result["score"]
        new_state["shd"] = action_result["shd"]
        new_state["nhd"] = action_result["nhd"]
        new_state["dag_variables"] = state["dag_variables"]
        new_state["desc_variables"] = state["desc_variables"]
        new_state["last_action"] = action
        new_state["last_reward"] = reward
        # Ensure init_nhd and init_shd are always present
        new_state["init_nhd"] = state.get("init_nhd")
        new_state["init_shd"] = state.get("init_shd")
        new_state["move_count"] = state.get("move_count", 0) + 1

        new_state["action_history"].append({
            "action": action,
            "reward": reward,
            "score": action_result["score"],
            "shd": action_result["shd"],
            "nhd": action_result["nhd"],
        })
        # Optionally, keep dag_variables, desc_variables, etc.
        return new_state

    def _format_result(self, state: dict) -> dict:
        """
        Formats the result of the LLM-driven hill climbing for output.
        """
        node_order = state.get("dag_variables", [])
        if len(node_order) == 0:
            raise StateUpdateError("DAG variables missing in the state")
        adj_matrix = nx.to_numpy_array(
            state["graph"], nodelist=node_order, weight=None, dtype=np.int8)

        action_history = state.get("action_history", [])
        shd = action_history[-1]["shd"] if action_history else state.get("init_shd")
        nhd = action_history[-1]["nhd"] if action_history else state.get("init_nhd")

        return {
            "Matrix": adj_matrix,
            "Score": state["score"],
            "Graph": state["graph"],
            "init_nhd": state.get("init_nhd"),
            "init_shd": state.get("init_shd"),
            "final_nhd": nhd,
            "final_shd": shd,
            "ActionHistory": state.get("action_history", []),
            "DagVariables": state.get("dag_variables", []),
            "DescVariables": state.get("desc_variables", ""),
            "MoveCount": state.get("move_count", 0),
        }

    def get_node_action_space_with_local_score(self, graph: DiscreteBayesianNetwork, node: str) -> list:
        """
        Returns all valid add/delete/reverse actions for the given node, each with the resulting parent set and local score.
        Does not modify the original graph.
        """
        actions = []
        current_parents = set(graph.get_parents(node))
        all_nodes = set(graph.nodes())
        # Try adding a parent
        for potential_parent in all_nodes - {node} - current_parents:
            new_graph = copy.deepcopy(graph)
            new_graph.add_edge(potential_parent, node)
            if nx.is_directed_acyclic_graph(new_graph):
                new_parents = set(current_parents | {potential_parent})
                local_score = self.scorer.local_score(node, list(new_parents))
                actions.append({
                    "type": "add",
                    "from": potential_parent,
                    "to": node,
                    "local_score": local_score,
                    "new_parents": list(new_parents),
                })
        # Try deleting a parent
        for parent in current_parents:
            new_graph = copy.deepcopy(graph)
            new_graph.remove_edge(parent, node)
            new_parents = set(current_parents - {parent})
            local_score = self.scorer.local_score(node, list(new_parents))
            actions.append({
                "type": "delete",
                "from": parent,
                "to": node,
                "local_score": local_score,
                "new_parents": list(new_parents),
            })
        # Try reversing edges (if allowed)
        for parent in current_parents:
            if graph.has_edge(node, parent):
                continue  # skip if reverse already exists
            new_graph = copy.deepcopy(graph)
            new_graph.remove_edge(parent, node)
            new_graph.add_edge(node, parent)
            if nx.is_directed_acyclic_graph(new_graph):
                # For the reversed edge, local BIC for node is with parent removed
                new_parents = set(current_parents - {parent})
                local_score = self.scorer.local_score(node, list(new_parents))
                actions.append({
                    "type": "reverse",
                    "from": parent,
                    "to": node,
                    "local_score": local_score,
                    "new_parents": list(new_parents),
                })
        return actions

    def _generate_legal_operations(self, current_graph, nodes, tabu_list, max_indegree, forbidden_edges, required_edges):
        legal_ops = []
        # Generate all possible add operations
        for i, j in itertools.permutations(nodes, 2):
            if current_graph.has_edge(i, j) or i == j:
                continue
            if (i, j) in forbidden_edges:
                continue
            added = False
            try:
                if not current_graph.has_edge(i, j):
                    current_graph.add_edge(i, j)
                    added = True
                if not nx.is_directed_acyclic_graph(current_graph):
                    if added and current_graph.has_edge(i, j):
                        current_graph.remove_edge(i, j)
                    continue
                if max_indegree is not None and len(list(current_graph.predecessors(j))) > max_indegree:
                    if added and current_graph.has_edge(i, j):
                        current_graph.remove_edge(i, j)
                    continue
            except Exception:
                if added and current_graph.has_edge(i, j):
                    current_graph.remove_edge(i, j)
                continue
            op = ('+', (i, j))
            if op not in tabu_list:
                legal_ops.append(op)
            if added and current_graph.has_edge(i, j):
                current_graph.remove_edge(i, j)
        # Generate all possible remove operations
        for i, j in list(current_graph.edges()):
            if (i, j) in required_edges:
                continue
            op = ('-', (i, j))
            if op not in tabu_list:
                legal_ops.append(op)
        # Generate all possible flip operations
        for i, j in list(current_graph.edges()):
            if current_graph.has_edge(j, i):
                continue  # skip if reverse already exists
            if (j, i) in forbidden_edges or (i, j) in required_edges:
                continue
            removed = False
            added = False
            try:
                if current_graph.has_edge(i, j):
                    current_graph.remove_edge(i, j)
                    removed = True
                if not current_graph.has_edge(j, i):
                    current_graph.add_edge(j, i)
                    added = True
                if not nx.is_directed_acyclic_graph(current_graph):
                    if added and current_graph.has_edge(j, i):
                        current_graph.remove_edge(j, i)
                    if removed and not current_graph.has_edge(i, j):
                        current_graph.add_edge(i, j)
                    continue
                if max_indegree is not None and len(list(current_graph.predecessors(i))) > max_indegree:
                    if added and current_graph.has_edge(j, i):
                        current_graph.remove_edge(j, i)
                    if removed and not current_graph.has_edge(i, j):
                        current_graph.add_edge(i, j)
                    continue
            except Exception:
                if added and current_graph.has_edge(j, i):
                    current_graph.remove_edge(j, i)
                if removed and not current_graph.has_edge(i, j):
                    current_graph.add_edge(i, j)
                continue
            op = ('flip', (i, j))
            if op not in tabu_list and ('flip', (j, i)) not in tabu_list:
                legal_ops.append(op)
            if added and current_graph.has_edge(j, i):
                current_graph.remove_edge(j, i)
            if removed and not current_graph.has_edge(i, j):
                current_graph.add_edge(i, j)
        return legal_ops

    def _compute_and_log_metrics(
        self,
        current_graph,
        iteration,
        current_score,
        confidence,
    ):
        pred_adj = nx.to_numpy_array(
            current_graph, nodelist=self.nodes, weight=None, dtype=int)
        ref_adj = nx.to_numpy_array(
            self.ref_graph, nodelist=self.nodes, weight=None, dtype=int)
        _, _, _, _, current_nhd, current_shd = evaluate_generation(
            ref_adj, pred_adj, self.logger, self.nodes,
        )

        self.logger.info(
            f"Iteration {iteration} - Score: {current_score}, SHD={current_shd}, NHD={current_nhd}, confidence={confidence}"
        )
        return current_nhd, current_shd

    def _track_state_and_action(
        self,
        current_graph, current_score, dag_variables, desc_variables,
        iteration, current_shd, current_nhd, op_type, parent, child, best_score_delta,
    ):
        # Track state
        state = {
            'graph': current_graph,
            'score': current_score,
            'dag_variables': dag_variables,
            'desc_variables': desc_variables,
            'move_count': iteration,
            'shd': current_shd,
            'nhd': current_nhd,
        }
        action = {'type': op_type, 'from': parent, 'to': child}
        reward = best_score_delta

        # Track action
        self.history.append(
            {'action': action, 'reward': reward, 'shd': current_shd, 'nhd': current_nhd})
        return state, action, reward

    def _log_step(
        self,
        metrics_history, action_history,
        iteration, current_score, current_nhd, current_shd,
        action, action_idx, reward, confidence, reasoning, legal_ops_len,
        terminated,
    ):
        metrics_history.append({
            "iteration": iteration,
            "score": current_score,
            "nhd": current_nhd,
            "shd": current_shd,
            "action": action,
            "action_idx": action_idx,
            "confidence": confidence,
            "reasoning": reasoning,
            "legal_ops": legal_ops_len,
            "terminated": terminated
        })
        action_history.append({
            "iteration": iteration,
            "action": action,
            "reward": reward,
            "shd": current_shd,
            "nhd": current_nhd,
            "terminated": terminated
        })

    def _finalize_result(
        self,
        state, action, current_graph, current_score, current_shd, current_nhd, iteration, reward, metrics_history,
    ):
        if state is not None:
            state = self.update_state(
                state,
                action,
                {'graph': current_graph, 'score': current_score,
                    'shd': current_shd, 'nhd': current_nhd},
                reward,
            )
            result = self._format_result(state)
        else:
            # fallback: format result from current_graph
            state = {
                'graph': current_graph,
                'score': current_score,
                'shd': current_shd,
                'nhd': current_nhd,
                'dag_variables': state['dag_variables'],
                'desc_variables': state['desc_variables'],
                'move_count': iteration,
            }
            result = self._format_result(state)
        # Add metrics history to result
        result["MetricsHistory"] = metrics_history
        result["ActionHistory"] = self.history  # Add action history to result
        return result

    def _apply_operation_in_place(self, state, best_op, tabu_list):
        """
        Applies the given operation (add, remove, flip) in-place to the graph in the state dict and updates the tabu list.
        """
        op_type, (parent, child) = best_op
        if op_type == '+':
            state['graph'].add_edge(parent, child)
            tabu_list.append(('-', (parent, child)))
        elif op_type == '-':
            if state['graph'].has_edge(parent, child):
                state['graph'].remove_edge(parent, child)
            tabu_list.append(('+', (parent, child)))
        elif op_type == 'flip':
            if state['graph'].has_edge(parent, child):
                state['graph'].remove_edge(parent, child)
            state['graph'].add_edge(child, parent)
            tabu_list.append(('flip', (parent, child)))
        else:
            raise ValueError(f"Unknown operation type: {op_type}")
